![text_summary_graphic](img/textsummarygraphicred.png)

# Building an NLP Extractive Text Summarization Model--from the Ground Up

## Overview  
The timing of this text summarization project coincides with a special era in Natural Language Processing (NLP), during sudden and enormous gains in model performance, and in particular, within Transfer Learning methods utilizing recently released pretrained models (e.g., BERT, XLNet, OpenAI). A couple of years ago, practitioners in Computer Vision experienced the beginning of a similar leap in model performance while NLP progress remained stagnant in comparison. But much has changed: As Sebastian Ruder writes:  

### "NLP's ImageNet moment has arrived."
  
This project has a two-fold aim: first, to provide informed research and the context of the current state of NLP in its present and dramatic transformation which seems to change on a weekly basis. Second, to produce my own text summarization algorithm as a way of conducting a deep study of the relationship between probabilities, tokenization, and word proximity--with the caveat that I'd be able to manipulate the algorithm without the hinderance of complex and hidden architectures featured in CNNs, LSTMs, and the new transformer designs. In other words, this project represents both research-based and practical applications in and around text summarization--not for benchmarking--as a pretrained model can provide that kind of top performance with ease. Rather, as a reference for practitioners who have little time to read academic papers and the technically inclined who wish to study the underpinnings of extractive text summarization.  

I sought to build an algorithm from the ground up and from which I could openly track, learn, code, and study code. In short, I wanted to see what was 'under the hood' before these high-performance models seal it shut for good. Distance metrics such as cosine similarity, and benchmark metrics that optimize for recall and precision, leave behind much of the baseline modeling process. For example, are all preprocessing routines equal in their ability to tokenize? How clean does clean text need to be for optimization? Can simple linear algebra help to search for and convert rare words into new, high-probability features?  

I'd wager that pretrained models (training from scratch takes 1-2 weeks on the fastest set of GPUs) are just on the horizon for professional use by data scientists in various organizations. They achieve state-of-the-art results on most Natural Language Processing "tasks" by standardized metrics, and given the choice, most organizations would not opt to train such a model from scratch for the sake of posterity. If you'd like to check out a showcase of the latest and greatest, I highly recommend [huggingFace's repository](https://github.com/huggingface/pytorch-transformers) which ports these models into PyTorch libraries (as opposed to Tensorflow).  

As such, I have no problem at all with their use. In fact, the release of these models is in accord with the general spirit of open source distribution from which so many benefit. In other words, I'm far from a desire to judge the new and shiny, and I'll likely benefit from the change, as well.
